{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np \n",
    "import os \n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torchaudio\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "import sys\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "import librosa\n",
    "from IPython.display import Audio\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn import metrics\n",
    "\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Audio_Data(Dataset):\n",
    "\n",
    "    def __init__(self, metadata, dir, transform, target_rate, num_samples, device):\n",
    "        self.metadata = pd.read_csv(metadata)\n",
    "        self.dir = dir\n",
    "        self.device = device\n",
    "        self.transform = transform.to(self.device)\n",
    "        self.target_rate = target_rate\n",
    "        self.num_samples = num_samples\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.metadata)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        audio_sample_path = self._get_audio_path(index)\n",
    "        label = self._get_audio_label(index)\n",
    "        signal, sr = torchaudio.load(audio_sample_path)\n",
    "        signal = signal.to(self.device)\n",
    "        signal = self._resample_if_necessary(signal, sr)\n",
    "        signal = self._mix_down_if_necessary(signal)\n",
    "        signal = self._cut_if_necessary(signal)\n",
    "        signal = self._right_pad_if_necessary(signal)\n",
    "        signal = self.transform(signal)\n",
    "        return signal, label\n",
    "\n",
    "    def _get_audio_path(self, index):\n",
    "        filename = f\"{self.metadata.iloc[index,0]}\"\n",
    "        audio_path = os.path.join(self.dir, filename)\n",
    "        return audio_path\n",
    "\n",
    "    def _get_audio_label(self, index):\n",
    "        return self.metadata.iloc[index,1]\n",
    "\n",
    "    def _resample_if_necessary(self, signal, sr):\n",
    "        if sr != self.target_rate:\n",
    "            resampler = torchaudio.transforms.Resample(sr, self.target_rate).cuda()\n",
    "            signal = resampler(signal)\n",
    "        return signal\n",
    "    \n",
    "    def _mix_down_if_necessary(self, signal):\n",
    "        if signal.shape[0] > 1:\n",
    "            signal = torch.mean(signal, dim=0, keepdim=True)\n",
    "        return signal \n",
    "\n",
    "    def _cut_if_necessary(self, signal):\n",
    "        if signal.shape[1] > self.num_samples:\n",
    "            signal = signal[:, :self.num_samples]\n",
    "        return signal\n",
    "\n",
    "    def _right_pad_if_necessary(self, signal):\n",
    "        signal_len = signal.shape[1]\n",
    "        if signal_len < self.num_samples:\n",
    "            difference = self.num_samples - signal_len\n",
    "            last_dim_padding = (0, difference)\n",
    "            signal = torch.nn.functional.pad(signal, last_dim_padding)\n",
    "        return signal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_spectrogram(specgram, title=None, ylabel=\"freq_bin\", ax=None):\n",
    "    if ax is None:\n",
    "        _, ax = plt.subplots(1, 1)\n",
    "    if title is not None:\n",
    "        ax.set_title(title)\n",
    "    ax.set_ylabel(ylabel)\n",
    "    ax.imshow(librosa.power_to_db(specgram), origin=\"lower\", aspect=\"auto\", interpolation=\"nearest\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DNN(nn.Module):\n",
    "    def __init__(self, num_fc_layers):\n",
    "        super(DNN, self).__init__()\n",
    "        self.num_fc_layers = num_fc_layers\n",
    "\n",
    "        # INPUT SHAPE = (BATCH SIZE, NUM_CHANNEL, NUM_MELS, NUM_FEATS)\n",
    "        # INPUT SHAPE = (BATCH SIZE, 1, 64, 219) WHEN SAMPLE RATE = 16000 AND DURATION = 7 SECS\n",
    "        self.conv_block = nn.Sequential(\n",
    "            nn.Conv2d(\n",
    "                in_channels = 1,         # Number of input channels; spectrograms will be treated as grayscale images\n",
    "                out_channels = 32,       # Number of filters in convolutional layer\n",
    "                kernel_size = 5,         \n",
    "            ),\n",
    "            nn.MaxPool2d(kernel_size = 2),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.ReLU(),                  \n",
    "            nn.Conv2d(\n",
    "                in_channels = 32,        # Number of input channels from previous convolution\n",
    "                out_channels = 64,       # Number of filters in convolutional layer\n",
    "                kernel_size = 5,         \n",
    "            ),\n",
    "            nn.MaxPool2d(kernel_size = 2),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(p = 0.25),\n",
    "            nn.Flatten(start_dim = 2)\n",
    "        )\n",
    "        # OUTPUT SHAPE = (BATCH SIZE, OUT_CHANNELS OF LAST CONV, FLATTEN)\n",
    "\n",
    "        self.gru  = nn.GRU(64, 128, num_layers = self.num_fc_layers, batch_first = True) # INPUT SIZE IS SAME AS NUMBER OF CHANNELS FROM LAST CNN LAYER\n",
    "        \n",
    "        self.fc_block = nn.Sequential(\n",
    "            nn.Linear(128,64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64,32),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(32,2)\n",
    "        )\n",
    "        \n",
    "    def forward(self, input_data):         \n",
    "        h0 = torch.zeros(self.num_fc_layers, input_data.shape[0], 128).to(device) # (num_layers, batch size, hidden size)\n",
    "        x = self.conv_block(input_data)\n",
    "        # print(x.shape)\n",
    "        x = x.reshape(-1, x.shape[2], x.shape[1]) # SHAPE: (BATCH SIZE, SEQ LENGTH or NUM FEATS, INPUT SIZE or NUM ROWS)\n",
    "        out, _ = self.gru(x, h0)\n",
    "        out = out[:, -1, :]\n",
    "        logits = F.sigmoid(self.fc_block(out))\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_data_loader(data, batch_size):\n",
    "    dataloader = DataLoader(data, batch_size=batch_size)\n",
    "    return dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, train_dataloader, val_dataloader, loss_fn, optimizer, device, epochs):           #Training the model\n",
    "    model.train()\n",
    "    train_loss = []\n",
    "    val_loss = []\n",
    "    for i in range(epochs):\n",
    "        print(f\"Epoch {i + 1}\")\n",
    "        epoch_train_loss, epoch_val_loss = train_single_epoch(model, train_dataloader, val_dataloader, loss_fn, optimizer, device)\n",
    "        train_loss.append(epoch_train_loss)\n",
    "        val_loss.append(epoch_val_loss)\n",
    "        print(\"--------------------------\")\n",
    "    print(\"Finished training\")\n",
    "    return train_loss, val_loss\n",
    "\n",
    "def train_single_epoch(model, train_dataloader, val_dataloader, loss_fn, optimizer, device):\n",
    "    epoch_train_loss = 0.0\n",
    "    num_train_batches = 0\n",
    "\n",
    "    epoch_val_loss = 0.0\n",
    "    num_val_batches = 0\n",
    "\n",
    "    for input, target in train_dataloader:\n",
    "        input, target = input.to(device), target.to(device)\n",
    "\n",
    "        #Calculate loss\n",
    "        prediction = model(input)\n",
    "        loss = loss_fn(prediction, target)\n",
    "\n",
    "        #Backpropagate error and update weights\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        epoch_train_loss += loss.item()\n",
    "        num_train_batches += 1\n",
    "        epoch_train_loss = epoch_train_loss / num_train_batches\n",
    "\n",
    "    print(f\"Train loss: {epoch_train_loss:.4f}\")\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for input, target in val_dataloader:\n",
    "            input, target = input.to(device), target.to(device)\n",
    "\n",
    "            #Calculate loss\n",
    "            prediction = model(input)\n",
    "            loss = loss_fn(prediction, target)\n",
    "\n",
    "            epoch_val_loss += loss.item()\n",
    "            num_val_batches += 1\n",
    "            epoch_val_loss = epoch_val_loss / num_val_batches\n",
    "        \n",
    "        print(f\"Val loss: {epoch_val_loss:.4f}\")\n",
    "\n",
    "    return epoch_train_loss, epoch_val_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_accuracy(data_loader, model):  \n",
    "    model.eval()\n",
    "    num_correct = 0\n",
    "    num_samples = 0 \n",
    "\n",
    "    with torch.no_grad():\n",
    "        for input, target in data_loader:\n",
    "            input = input.to(device=device)\n",
    "            target = target.to(device=device)\n",
    "\n",
    "            scores = model(input)\n",
    "            _, predictions = scores.max(1)\n",
    "            num_correct += (predictions == target).sum()\n",
    "            num_samples += predictions.size(0)\n",
    "        \n",
    "        print(\"Num of samples \", num_samples)\n",
    "        print(\"Num of correct \", int(num_correct)) \n",
    "        print(\"Accuracy: \", int((num_correct/num_samples)*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(model, input, target):  # Make predictions with the model\n",
    "    model.eval()\n",
    "    class_mapping = ['LJ', 'AI']\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        predictions = model(input)\n",
    "        predicted_index = predictions[0].argmax(0)\n",
    "        predicted = class_mapping[predicted_index]\n",
    "        expected = class_mapping[target]\n",
    "    return predicted, expected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def confusion_matrix(audio_data, model):\n",
    "    TP, FP, FN, TN = 0, 0, 0, 0\n",
    "\n",
    "    for i in range(len(audio_data)):\n",
    "        input, target = audio_data[i][0], audio_data[i][1]  # Sample from dataset: [batch size, number of channels, frequency, time]\n",
    "        input.unsqueeze_(0)\n",
    "\n",
    "        predicted, expected = predict(model, input, target)  # Making a prediction with the model\n",
    "        # print(\"Sample:\", i)\n",
    "        # print(f\"Predicted: '{predicted}', expected: '{expected}'\")\n",
    "\n",
    "        if (expected == 'AI') & (predicted == 'AI'):\n",
    "            TP += 1\n",
    "        elif (expected == 'LJ') & (predicted == 'AI'):\n",
    "            FP += 1\n",
    "        elif (expected == 'AI') & (predicted == 'LJ'): \n",
    "            FN += 1 \n",
    "        elif (expected == 'LJ') & (predicted == 'LJ'):\n",
    "            TN += 1   \n",
    "            \n",
    "    print(\"Confusion Matrix\")\n",
    "    print(TP, FP)\n",
    "    print(FN, TN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluation_matrix(model, audio_data):\n",
    "    TP, FP, FN, TN = 0, 0, 0, 0\n",
    "\n",
    "    for i in range(len(audio_data)):\n",
    "        input, target = audio_data[i][0], audio_data[i][1]  # Sample from dataset: [batch size, number of channels, frequency, time]\n",
    "        input.unsqueeze_(0)\n",
    "\n",
    "        predicted, expected = predict(model, input, target)  # Making a prediction with the model\n",
    "        # print(\"Sample:\", i)\n",
    "        # print(f\"Predicted: '{predicted}', expected: '{expected}'\")\n",
    "\n",
    "        if (expected == 'AI') & (predicted == 'AI'):\n",
    "            TP += 1\n",
    "        elif (expected == 'LJ') & (predicted == 'AI'):\n",
    "            FP += 1\n",
    "        elif (expected == 'AI') & (predicted == 'LJ'): \n",
    "            FN += 1 \n",
    "        elif (expected == 'LJ') & (predicted == 'LJ'):\n",
    "            TN += 1   \n",
    "\n",
    "    print(\"Confusion Matrix\")\n",
    "    print(TP, FP)\n",
    "    print(FN, TN)\n",
    "    precision = TP / (TP+FP)\n",
    "    recall = TP / (TP+FN)\n",
    "    accuracy = (TP+TN) / (TP+FP+FN+TN)\n",
    "    f1_score = (2 * (precision * recall)) / (precision + recall)\n",
    "    print(\"Accuracy:\", accuracy)\n",
    "    print(\"Precision:\", precision)\n",
    "    print(\"Recall:\", recall)\n",
    "    print(\"F1-score:\", f1_score)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_roc_auc_score(model, audio_data):    \n",
    "    model.eval()\n",
    "    y_true = []\n",
    "    y_pred = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for i in range(len(audio_data)):\n",
    "            input, target = audio_data[i][0], audio_data[i][1]  #Sample from dataset: [batch size, number of channels, frequency, time]\n",
    "            input.unsqueeze_(0)\n",
    "            y_true.append(target)\n",
    "\n",
    "            predictions = model(input)\n",
    "            predicted = predictions[0].argmax(0)\n",
    "            y_pred.append(predicted.cpu().numpy())\n",
    "    \n",
    "    y_true = np.array(y_true)\n",
    "    # print(y_pred)\n",
    "    # print(y_true)\n",
    "    roc_auc = roc_auc_score(y_true, y_pred)\n",
    "\n",
    "    return roc_auc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_roc(model, audio_data):    \n",
    "    model.eval()\n",
    "    y_true = []\n",
    "    y_pred = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for i in range(len(audio_data)):\n",
    "            input, target = audio_data[i][0], audio_data[i][1]  #Sample from dataset: [batch size, number of channels, frequency, time]\n",
    "            input.unsqueeze_(0)\n",
    "            y_true.append(target)\n",
    "\n",
    "            predictions = model(input)\n",
    "            predicted = predictions[0].argmax(0)\n",
    "            y_pred.append(predicted.cpu().numpy())\n",
    "    \n",
    "    y_true = np.array(y_true)\n",
    "    # print(y_pred)\n",
    "    # print(y_true)\n",
    "    nn_fpr, nn_tpr, nn_thresholds = metrics.roc_curve(y_true, y_pred)\n",
    "\n",
    "    return nn_fpr, nn_tpr\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyper-parameters for Melspectrogram transformation \n",
    "\n",
    "sample_rate = 22050\n",
    "num_samples = 154350\n",
    "n_fft = 1024\n",
    "win_length = None\n",
    "hop_length = 512\n",
    "n_mels = 64\n",
    "\n",
    "mel_spec = torchaudio.transforms.MelSpectrogram(\n",
    "    sample_rate=sample_rate,\n",
    "    n_fft=n_fft,\n",
    "    win_length=win_length,\n",
    "    hop_length=hop_length,\n",
    "    center=True,\n",
    "    pad_mode=\"reflect\",\n",
    "    power=2.0,\n",
    "    norm=\"slaney\",\n",
    "    n_mels=n_mels,\n",
    "    mel_scale=\"htk\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataloader initialization\n",
    "# Audio_Data class object is initialized for each dataloader\n",
    "# Train, Validation, and Test dataloaders were used in original project\n",
    "\n",
    "Train_metadata_path = 'insert training metadata file path'\n",
    "Train_dir_path = 'insert training dataset path' \n",
    "\n",
    "Valid_metadata_path = 'insert validation metadata file path'\n",
    "Valid_dir_path = 'insert validation dataset path' \n",
    "\n",
    "Test_metadata_path = 'insert testing metadata file path'\n",
    "Test_dir_path = 'insert testing dataset path' \n",
    "\n",
    "batch_size = 128\n",
    "\n",
    "# Training audio data instance\n",
    "Train_audio_data = Audio_Data(Train_metadata_path, Train_dir_path, mel_spec, sample_rate, num_samples, device)\n",
    "print(\"Training set length:\", f\"{len(Train_audio_data)}\")\n",
    "train_dataloader = create_data_loader(Train_audio_data, batch_size)\n",
    "\n",
    "# Validation audio data instance\n",
    "Valid_audio_data = Audio_Data(Valid_metadata_path, Valid_dir_path, mel_spec, sample_rate, num_samples, device)\n",
    "print(\"Validation set length:\", f\"{len(Valid_audio_data)}\")\n",
    "val_dataloader = create_data_loader(Valid_audio_data, batch_size)\n",
    "\n",
    "# Testing audio data instance\n",
    "Test_audio_data = Audio_Data(Test_metadata_path, Test_dir_path, mel_spec, sample_rate, num_samples, device)\n",
    "print(\"Test set length:\", f\"{len(Test_audio_data)}\")\n",
    "test_dataloader = create_data_loader(Test_audio_data, batch_size) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = nn.CrossEntropyLoss()   \n",
    "\n",
    "model = DNN(4).to(device)  # initializing model, parameter passed determines number of FC layers\n",
    "model_opt = optim.Adam(model.parameters(), lr = 0.0001)\n",
    "print(model)\n",
    "\n",
    "check_accuracy(train_dataloader, model)\n",
    "check_accuracy(val_dataloader, model)\n",
    "check_accuracy(test_dataloader, model)\n",
    "\n",
    "train_loss_results = []\n",
    "val_loss_results = []\n",
    "\n",
    "EPOCHS = 100\n",
    "\n",
    "train_loss_results = []\n",
    "val_loss_results = []\n",
    "\n",
    "train_loss, val_loss = train(model, train_dataloader, val_dataloader, loss_fn, model_opt, device, EPOCHS)  # Training the model\n",
    "\n",
    "train_loss_results.append(train_loss)\n",
    "val_loss_results.append(val_loss)\n",
    "\n",
    "model_name = 'name.pth'  # provide model name for each model trained\n",
    "torch.save(model.state_dict(), model_name)               \n",
    "print(\"Trained neural network saved at\", model_name)\n",
    "\n",
    "model_train_loss = []\n",
    "model_val_loss = []\n",
    "\n",
    "for i in range(0,EPOCHS):\n",
    "    model_train_loss.append(float(train_loss_results[0][i]))\n",
    "    model_val_loss.append(float(val_loss_results[0][i]))\n",
    "\n",
    "plt.plot(model_train_loss, label='Train Loss')\n",
    "plt.plot(model_val_loss, label='Validation Loss')\n",
    "plt.legend(loc='upper right')\n",
    "plt.title('Training Performance')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.show()\n",
    "\n",
    "print('Evaluation Matrix')\n",
    "evaluation_matrix(model, Test_audio_data)\n",
    "score = get_roc_auc_score(model, Test_audio_data)\n",
    "print('ROC_AUC_Score:', score)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# input, target = Test_audio_data[350][0], Test_audio_data[350][1] # [batch size, num_channels, fr, time]\n",
    "# input.unsqueeze_(0)\n",
    "\n",
    "# # make an inference\n",
    "# predicted, expected = predict(model, input, target)\n",
    "# print(f\"Predicted: '{predicted}', expected: '{expected}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "saved_model = DNN(4).to(device)\n",
    "state_dict = torch.load(\"insert path of trained model\")\n",
    "saved_model.load_state_dict(state_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluation_matrix(saved_model, Test_audio_data)\n",
    "score = get_roc_auc_score(saved_model, Test_audio_data)\n",
    "print('ROC_AUC_Score:', score)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
